{"cells":[{"cell_type":"markdown","metadata":{},"source":["### This code was run on Deepnote with a lot of RAM and computational resources."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"cb821b78bf874244874993c64b8982d7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3398,"execution_start":1703008606674,"source_hash":null},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-12-19 17:56:46.767524: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","/shared-libs/python3.8/py/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import tensorflow as tf\n","from transformers import TFAutoModel, AutoTokenizer\n","import datasets\n","import pandas as pd\n","import json\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"72db49097c1b416caa675e0e8ca0959a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1776,"execution_start":1703008610070,"source_hash":null},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-12-19 17:56:50.909230: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}],"source":["model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"af922fbb7a024b86b95154688b653480","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2395,"execution_start":1703008611844,"source_hash":null},"outputs":[],"source":["train_df = pd.read_json('subtaskA_train_monolingual.jsonl', lines=True)\n","test_df = pd.read_json('subtaskA_dev_monolingual.jsonl', lines=True)\n","\n","shuffled_train = train_df.sample(frac=1.0, random_state=42)\n","shuffled_test  = test_df.sample(frac=1.0, random_state=42)\n","\n","train_dataset = datasets.Dataset.from_dict(shuffled_train[:10000]) # 10k samples, since it takes a lot of time to train\n","test_dataset = datasets.Dataset.from_dict(shuffled_test)\n","complete_dict = datasets.DatasetDict({\"train\":train_dataset,\"test\":test_dataset})"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"3cd4b4bc68bc4629a8b106d584a21219","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":11,"execution_start":1703008614241,"source_hash":null},"outputs":[],"source":["def tokenize(batch):\n","    return tokenizer(batch[\"text\"], padding=True, truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ba2a5361c7f1416d90c26fe5b415eaf1","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4545,"execution_start":1703008614244,"source_hash":null},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 10000/10000 [00:03<00:00, 3117.12 examples/s]\n","Map: 100%|██████████| 5000/5000 [00:01<00:00, 3794.73 examples/s]\n"]}],"source":["complete_encoded = complete_dict.map(tokenize, batched=True, batch_size=None) # tokenize the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"21c79324dba54b398b0b3a3b015c098b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":747,"execution_start":1703008736969,"source_hash":null},"outputs":[],"source":["input_ids = complete_encoded[\"test\"][\"input_ids\"]\n","ids = complete_encoded[\"test\"][\"id\"]\n","\n","input_ids_tuples = [tuple(row) for row in input_ids]\n","\n","id_map = dict(zip(input_ids_tuples, ids)) # make a map of input_ids and id to be able to map the predictions to the correct id"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ebc32653e0754d728347617722e1a660","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":145,"execution_start":1703008741380,"source_hash":null},"outputs":[],"source":["# This code is taken from Keggle https://www.kaggle.com/code/pritishmishra/fine-tune-bert-for-text-classification/notebook?scriptVersionId=116951029\n","complete_encoded.set_format('tf', \n","                            columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n","\n","# setting BATCH_SIZE to 64.\n","BATCH_SIZE = 64\n","\n","def order(inp):\n","    '''\n","    This function will group all the inputs of BERT\n","    into a single dictionary and then output it with\n","    labels.\n","    '''\n","    data = list(inp.values())\n","    return {\n","        'input_ids': data[1],\n","        'attention_mask': data[2],\n","        'token_type_ids': data[3]\n","    }, data[0]\n","    \n","\n","train_dataset = tf.data.Dataset.from_tensor_slices(complete_encoded['train'][:])\n","train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000)\n","train_dataset = train_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","\n","test_dataset = tf.data.Dataset.from_tensor_slices(complete_encoded['test'][:])\n","test_dataset = test_dataset.batch(BATCH_SIZE)\n","test_dataset = test_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"a50ef26767f2483ab0c31dd9a9646c08","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":13,"execution_start":1703008744734,"source_hash":null},"outputs":[],"source":["# This code is taken from Keggle https://www.kaggle.com/code/pritishmishra/fine-tune-bert-for-text-classification/notebook?scriptVersionId=116951029\n","\n","class BERTForClassification(tf.keras.Model):\n","    \n","    def __init__(self, bert_model, num_classes):\n","        super().__init__()\n","        self.bert = bert_model\n","        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax')\n","        \n","    def call(self, inputs):\n","        x = self.bert(inputs)[1]\n","        return self.fc(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"8b081335a47545abaf0d316aa86bc923","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":30,"execution_start":1703008747129,"source_hash":null},"outputs":[],"source":["# This code is taken from Keggle https://www.kaggle.com/code/pritishmishra/fine-tune-bert-for-text-classification/notebook?scriptVersionId=116951029\n","\n","classifier = BERTForClassification(model, num_classes=2)\n","\n","classifier.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","    metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"211e0cd8e901499d8467aa62f335c835","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4279148,"execution_start":1703008749515,"source_hash":null},"outputs":[{"name":"stdout","output_type":"stream","text":["157/157 [==============================] - 4279s 27s/step - loss: 0.4109 - accuracy: 0.7917\n"]}],"source":["history = classifier.fit(\n","    train_dataset,\n","    epochs=1\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"378caaa441dd4b90b6ae8d159a233d7d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":324,"execution_start":1703013028660,"source_hash":null},"outputs":[],"source":["all_input_ids = test_dataset.map(lambda x, y: x['input_ids'])\n","\n","all_input_ids_numpy = np.concatenate(list(all_input_ids.as_numpy_iterator()))\n","\n","ids = []\n","for input_id in all_input_ids_numpy:\n","    specific_input_id = tuple(np.array(input_id))  \n","    ids.append(id_map.get(specific_input_id, \"Not Found\"))\n","\n","# translate input_ids to id to match predictions with the correct id"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"6113ea494ae24828a0d8988a7186f69a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":664373,"execution_start":1703013028985,"source_hash":null},"outputs":[{"name":"stdout","output_type":"stream","text":["79/79 [==============================] - 664s 8s/step\n","Predictions saved to predictions.jsonl\n"]}],"source":["# Make predictions and save them to a json file in the correct format.\n","\n","predictions = classifier.predict(test_dataset)\n","\n","predicted_labels = tf.argmax(predictions, axis=1).numpy()\n","\n","results = [{\"id\": int(i), \"label\": int(label)} for i, label in zip(ids, predicted_labels)]\n","\n","output_file = 'predictions.jsonl'\n","with open(output_file, 'w', encoding='utf-8') as f:\n","    for result in results:\n","        f.write(json.dumps(result) + '\\n')\n","\n","print(f'Predictions saved to {output_file}')"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4fbf66f6-adcb-47c6-bdee-342fcaac18fa' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"b2ae9cefa59044dd82ec4ee396ed62c6","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
